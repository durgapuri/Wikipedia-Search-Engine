{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Merger.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"16IiKrLpUDUqYe3zoHLra5XhG8Xs0sOV6","authorship_tag":"ABX9TyMvIqAgQW47o68mupC1WOS3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XzQm8yHpZv-Q","executionInfo":{"status":"ok","timestamp":1599385204274,"user_tz":-330,"elapsed":1025,"user":{"displayName":"jyoti gambhir","photoUrl":"","userId":"05165840996487063875"}},"outputId":"9e43bd4a-0f50-43a8-ad6f-3fdf8b61e13d","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import xml.sax\n","import sys\n","sys.path.append('/content/drive/My Drive/IRE')\n","import collections\n","import os\n","import os.path\n","import re\n","import re\n","from collections import defaultdict\n","import Stemmer\n","# from nltk.stem.snowball import SnowballStemmer\n","# from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","import timeit\n","from parsedata import Parser\n","import bz2\n","import glob\n","from heapq import heappush,heappop"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QcB-WMygmDDR","executionInfo":{"status":"ok","timestamp":1599371488661,"user_tz":-330,"elapsed":14156,"user":{"displayName":"jyoti gambhir","photoUrl":"","userId":"05165840996487063875"}},"outputId":"f151991d-c8be-4476-e72a-0374d9b45c51","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["!pip3 install PyStemmer\n","!pip3 install nltk"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting PyStemmer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/b2/c3aeebfe4a60256ddb72257e750a94c26c3085f017b7e58c860d5aa91432/PyStemmer-2.0.1.tar.gz (559kB)\n","\r\u001b[K     |▋                               | 10kB 11.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 2.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyStemmer\n","  Building wheel for PyStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyStemmer: filename=PyStemmer-2.0.1-cp36-cp36m-linux_x86_64.whl size=423413 sha256=81cee3cf66881e265c552c4f7eb4be15ef25ae73a58fa09aff59382102691d25\n","  Stored in directory: /root/.cache/pip/wheels/f3/3c/11/ee323a09706e17a649c2730bd8819b06e887411ff7507acf7a\n","Successfully built PyStemmer\n","Installing collected packages: PyStemmer\n","Successfully installed PyStemmer-2.0.1\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EmGv6D2VmF-L"},"source":["dump_folder=\"\"\n","dump_files = list()\n","file_status = list()\n","file_count = 0\n","file_pointers = dict()\n","heap_words = list()\n","word_posting_list = dict()\n","global_dict = dict()\n","chunk_size = 170000\n","primary_file_count = 0\n","secondary_index = dict()\n","\n","# get files from dump created by indexer(50000 doc in each)\n","def get_files_from_dump(dump_folder):\n","  global dump_files, file_status, file_count\n","  dump_files = glob.glob(dump_folder+'/indexfile*')\n","  # print(dump_files)\n","  # dump_files = dump_files[:6]\n","  dump_files.sort()\n","  # print(dump_files)\n","  file_count = len(dump_files)\n","  file_status = [0]*file_count\n","\n","# from each index file pick up frist word and insert in heap if not present\n","def initialize_parameters():\n","  global file_status, file_pointers, word_posting_list\n","  for filenum in range(file_count):\n","    # print(dump_files[filenum])\n","    file_status[filenum] = 1\n","    file_pointers[filenum] = open(dump_files[filenum], 'r')\n","    word_posting_list[filenum] = file_pointers[filenum].readline().split(':')\n","    word = word_posting_list[filenum][0]\n","    # print(word)\n","    if word not in heap_words:\n","      # print('pushed', word)\n","      heappush(heap_words, word)\n","\n","def push_word_in_heap(filenum, next_list):\n","  global heap_words, word_posting_list\n","  word_posting_list[filenum] = next_list.split(':')\n","  word = word_posting_list[filenum][0]\n","  if word not in heap_words:\n","    heappush(heap_words, word)\n","\n","def update_status(filenum):\n","  global file_pointers, file_status\n","  file_pointers[filenum].close()\n","  file_status[filenum] = 0\n","\n","# global dict contains chunksize words --- flush to primary file and write first word with primary file name to secondary index\n","def create_primary_index_file():\n","  global primary_file_count, global_dict\n","  primary_file_count += 1\n","  insert_in_sec = True\n","  primary_index_file = '/content/primaryfile_'+str(primary_file_count)+'.txt'\n","  \n","  with open(primary_index_file, \"w\") as of:\n","    for word in sorted(global_dict):\n","      if insert_in_sec:\n","        secondary_index[word] = 'primaryfile_' + str(primary_file_count)+'.txt'\n","        insert_in_sec = False\n","      # print(word + ':' + '|'.join(global_dict[word]))\n","      of.write(word + ':' + '|'.join(global_dict[word])+'\\n')\n","  of.close()\n","\n","def create_secondary_index_file():\n","  global secondary_index\n","  secondary_index_file = '/content/secondaryfile.txt'\n","  with open(secondary_index_file, \"w\") as of:\n","    for i in sorted(secondary_index):\n","      of.write(i + \":\" + secondary_index[i] + '\\n')\n","  of.close()\n","\n","#k way merge -- pick first word from heap check for doc with first word and append the posting list for that word in global dict\n","def merge_index_files():\n","  global file_status, file_pointers, word_posting_list, global_dict, chunk_size\n","  currwords = 0\n","  while file_status.count(0) < file_count:\n","\n","    topval = heappop(heap_words)\n","    currwords += 1\n","    global_dict[topval] = []\n","    for i in range(file_count):\n","      if file_status[i] == 1 and word_posting_list[i][0] == topval:\n","        posting_list = word_posting_list[i][1][:-1]\n","        # print(\"before \", posting_list)\n","        global_dict[topval].append(posting_list)\n","        # print(\"after \", global_dict[topval])\n","        next_list = file_pointers[i].readline()\n","\n","        if next_list:\n","          push_word_in_heap(i, next_list)\n","        else:\n","          update_status(i)\n","    \n","    if currwords >= chunk_size:\n","      create_primary_index_file()\n","      currwords = 0\n","      global_dict.clear() \n","      # return\n","      # sys.exit()\n","\n","\n","\n","\n","if (__name__ == \"__main__\"):\n","\n","  dump_folder = \"/content\"\n","  get_files_from_dump(dump_folder)\n","  # print(file_count)\n","  initialize_parameters()\n","  merge_index_files()\n","  if bool(global_dict):\n","    create_primary_index_file()\n","  # flush secondary index to secondary file\n","  create_secondary_index_file()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsKcwc-ymdDA"},"source":[""],"execution_count":null,"outputs":[]}]}