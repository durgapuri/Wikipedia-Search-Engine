{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"search.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1cN7co7pr0q7vAYCTSEqBQYR99gvvFH6u","authorship_tag":"ABX9TyMe3WMuGUztmC7lTBSPwJcG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XksrCdPdwX8B","executionInfo":{"status":"ok","timestamp":1601276713643,"user_tz":-330,"elapsed":2458,"user":{"displayName":"jyoti gambhir","photoUrl":"","userId":"05165840996487063875"}},"outputId":"df5400c5-6632-4089-b112-ed0b35996c39","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import xml.sax\n","import sys\n","sys.path.append('/content/drive/My Drive/IRE')\n","import collections\n","import os\n","import os.path\n","import re\n","from collections import defaultdict\n","import Stemmer\n","# from nltk.stem.snowball import SnowballStemmer\n","# from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","import timeit\n","from parsedata import Parser\n","from bisect import bisect\n","from math import log10\n","from operator import itemgetter"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UXztHAgTza0Q","executionInfo":{"status":"ok","timestamp":1601276709220,"user_tz":-330,"elapsed":14603,"user":{"displayName":"jyoti gambhir","photoUrl":"","userId":"05165840996487063875"}},"outputId":"f4955316-c425-4b17-adfd-ce9b99b876cd","colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["!pip3 install PyStemmer\n","!pip3 install nltk"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting PyStemmer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/b2/c3aeebfe4a60256ddb72257e750a94c26c3085f017b7e58c860d5aa91432/PyStemmer-2.0.1.tar.gz (559kB)\n","\u001b[K     |████████████████████████████████| 563kB 2.9MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyStemmer\n","  Building wheel for PyStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyStemmer: filename=PyStemmer-2.0.1-cp36-cp36m-linux_x86_64.whl size=423789 sha256=8c9a53445a60a827f792eb5a3e60dae5d40e63d640bfc0c34c7fbe1d41c7ca93\n","  Stored in directory: /root/.cache/pip/wheels/f3/3c/11/ee323a09706e17a649c2730bd8819b06e887411ff7507acf7a\n","Successfully built PyStemmer\n","Installing collected packages: PyStemmer\n","Successfully installed PyStemmer-2.0.1\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SAoYPXx_zhN1","executionInfo":{"status":"ok","timestamp":1601287253487,"user_tz":-330,"elapsed":248810,"user":{"displayName":"jyoti gambhir","photoUrl":"","userId":"05165840996487063875"}},"outputId":"de2535fe-c05a-41fc-eaf7-69beafb290e1","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["index_file_path = \"\"\n","query_file_path = \"\"\n","output_file_path = \"\"\n","total_docs = 0\n","id_title = dict()\n","secondary_index = []\n","queries = []\n","outputs = []\n","query_result = dict()\n","\n","def process_title_file(id_title_file_path):\n","  global total_docs, id_title\n","  with open(id_title_file_path, 'r') as inf:\n","    for l in inf:\n","      id, title = l.split(\":\",1)\n","      id_title[id] = title\n","      total_docs += 1\n","\n","def read_secondary_index(secondary_index_file_path):\n","  global secondary_index\n","  with open(secondary_index_file_path, 'r') as inf:\n","    for l in inf:\n","      secondary_index.append(l.split(\":\")[0])\n","\n","def fetch_data(check_match, data_in_file):\n","  start_in = check_match.start()\n","  find_in = start_in+1\n","  end_in = data_in_file.find('\\n', find_in)\n","  get_data = data_in_file[start_in:end_in]\n","  return get_data.split(':')[1].split('|')\n","\n","def read_primary_index_file(filename, word):\n","  primary_file = open(filename, 'r')\n","  data_in_file = primary_file.read()\n","  check_match = re.search('^'+word+':', data_in_file, re.M)\n","  if check_match:\n","    fetched_data = fetch_data(check_match, data_in_file)    ##get posting list for that word from the file (list of docs#t2+i4)\n","    return fetched_data\n","  else:\n","    return []\n","\n","\n","def get_index_field_query(tag, word):\n","  global secondary_index, query_result, index_file_path, total_docs\n","  # print(word)\n","  # print(secondary_index)\n","  # print(\"word processed \",tag, word)\n","  indx = bisect(secondary_index, word)          ## search for the word in secondary list\n","  entire_list = list()\n","  if indx > 0:\n","    entire_list = read_primary_index_file(index_file_path+'primaryfile_'+str(indx)+'.txt', word)    ## get positing list corresponding \n","                                                                                                   ## to that word from primary file\n","  df = len(entire_list)\n","  idf = log10(total_docs/(1+df))          ##cal idf score\n","\n","  for lt in entire_list:\n","    wt_freq = 0\n","    docid, posting_list = lt.split('#')\n","    docid = docid[1:]\n","    for cat in posting_list.split('+'): #get all tags for that doc\n","      post_tag = cat[0]\n","      freq = int(cat[1:])\n","      if post_tag == tag:               ## if queried tag is same as posting tag\n","        wt_freq += freq*10000         \n","      else:\n","        wt_freq += freq\n","\n","    tfidf = float(log10(1+wt_freq)*float(idf))        ## cal tfidf score\n","    if docid not in query_result:\n","      query_result[docid] = tfidf\n","    else:\n","      query_result[docid] += tfidf                    ## update tfidf score for the docid\n","    # print(docid, query_result[docid])\n","         \n","\n","\n","def process_field_query(tag, words):\n","  ex = re.compile(r'[\\_]', re.DOTALL)\n","  words_ex = ex.sub(' ', words)\n","  words_list = re.findall(\"\\d+|[\\w]+\", str(words_ex))     ##cleaning\n","  words_list = clean_words(words_list)\n","  for w in words_list:                          ## process for each word corresponding to queried tag\n","    get_index_field_query(tag, w.casefold())\n","\n","def get_k_documents(k):\n","  global query_result, id_title, outputs\n","  cnt = 0\n","  dic_rev_sorted = dict(sorted(query_result.items(), key=itemgetter(1), reverse=True))  ## sort dict in reverse order of tfidf\n"," \n","\n","  for doc,title in dic_rev_sorted.items():          ## get k doc ids and their title\n","    outputs.append(doc+', '+id_title[doc])\n","    cnt += 1\n","    if cnt == k:\n","      break\n","  query_result.clear()\n","\n","def process_multi_field_query(query):\n","  words = query.split(' ')\n","  local_dict = {}\n","  for i in words:\n","    if ':' in i:\n","      tag, w = i.split(':')\n","      local_dict[tag] = [w.casefold().strip()]\n","    else:\n","      local_dict[tag].append(i.casefold())\n","  return local_dict\n","\n","def clean_words(words):\n","  #words = words.casefold()\n","  stop_words = set(stopwords.words('english'))\n","  tokens = [t for t in words if t not in stop_words]\n","  stemmer = Stemmer.Stemmer('english')\n","  tokens = [stemmer.stemWord(w) for w in tokens]\n","  return tokens\n","\n","def get_index_normal_query(word):\n","  indx = bisect(secondary_index, word)              ##search for word in secondary\n","  entire_list = list()\n","  if indx > 0:\n","    entire_list = read_primary_index_file(index_file_path+'primaryfile_'+str(indx)+'.txt', word)    ## get positng list\n","  \n","  df = len(entire_list)\n","  idf = log10(total_docs/(1+df))          ## cal idf\n","\n","  li = {'t':100000, 'i':1, 'b':0.5, 'c':1, 'l':1, 'r':1}\n","\n","  for lt in entire_list:\n","    wt_freq = 0\n","    docid, posting_list = lt.split('#')\n","    docid = docid[1:]\n","    # if docid == '3433930':\n","    #   print(posting_list)\n","    for cat in posting_list.split('+'):\n","      post_tag = cat[0]\n","      freq = int(cat[1:])\n","      wt_freq += li[post_tag]*freq                ## weighted freq\n","\n","    tfidf = float(log10(1+wt_freq)*float(idf))    ## cal tfidf\n","    if docid not in query_result:\n","      query_result[docid] = tfidf\n","    else:\n","      query_result[docid] += tfidf          ## update tfidf score\n","    # if word == '2019':\n","    #   print(docid, query_result[docid])\n","\n","\n","def process_normal_query(words):\n","  words = words.lower()\n","  ex = re.compile(r'[\\_]', re.DOTALL)\n","  words_ex = ex.sub(' ', words)\n","  words_list = re.findall(\"\\d+|[\\w]+\", str(words_ex))\n","  words_list = clean_words(words_list)\n","  # print(words_list)\n","  for word in words_list:                 ## for each word in normal query\n","    get_index_normal_query(word)\n","    # print(\"processed \", word)\n","    # print(query_result)\n","    \n","  \n","  \n","def search_queries():\n","  global outputs, queries\n","  li = ['t:', 'i:', 'b:', 'c:', 'l:', 'r:']\n","  for q in queries:\n","      # outputs.append(q)\n","      print(q)\n","      #get count of req docs\n","      num,qu = q.split(',',1)\n","      num = int(num)\n","      qu = qu.strip()\n","      start = timeit.default_timer()\n","      ## check for field query\n","      if any(val in qu for val in li):\n","        processed_dictionary = process_multi_field_query(qu)  ## get words for each tag\n","        for tag,words in processed_dictionary.items():  ## process for each tag\n","          # print(tag, words)\n","          process_field_query(tag, ' '.join(words))\n","          # print(\"tfidf score after \", tag)\n","          # for k,v in query_result.items():\n","          #   print(k,v)\n","        get_k_documents(num)\n","        # print(outputs)\n","      else:\n","        process_normal_query(qu)\n","        get_k_documents(num)\n","        # print(outputs)\n","      stop = timeit.default_timer()\n","      outputs.append(str(stop-start)+' '+str((stop-start)/num)+'\\n')\n","      print(str(stop-start)+' '+str((stop-start)/num)+'\\n')\n","      outputs.append('\\n')\n","\n","if __name__==\"__main__\":\n","  global index_file_path, query_file_path, output_file_path, queries \n","  index_file_path = \"/content/drive/My Drive/IRE/InvertedIndexFiles/\"\n","  query_file_path = '/content/drive/My Drive/IRE/2019201032_queries1.txt'\n","  output_file_path = '/content/drive/My Drive/IRE/serachtest/output.txt'\n","  id_title_file_path = '/content/drive/My Drive/IRE/IndexFiles/id_title.txt'\n","\n","  # create title id map from file title id file\n","  process_title_file(id_title_file_path)\n","  # print(total_docs)\n","  # print(id_title[str(1)])\n","\n","  #read words from secondary index file\n","  secondary_index_file_path = '/content/drive/My Drive/IRE/InvertedIndexFiles/secondaryfile.txt'\n","  read_secondary_index(secondary_index_file_path)\n","\n","  queries = open(query_file_path, 'r').readlines()\n","  search_queries()\n","  \n","  # flush output to the file\n","  # print(\"time\", stop-start)\n","  with open(output_file_path, 'w') as of:\n","    for line in outputs:\n","      of.write(line.strip()+'\\n')\n","\n","  \n","\n","  \n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["5, t:catcher in the rye i:axl rose b:madagascar c:2008 albums\n","\n","32.453523965001295 6.490704793000259\n","\n","20, Gandhi 1982\n","\n","3.8077325589983957 0.19038662794991978\n","\n","10, Blade Runner\n","\n","4.368284473999665 0.4368284473999665\n","\n","10, b:William Owen c:English songwriters\n","\n","15.136559353000848 1.513655935300085\n","\n","20, t:Virgin Islands r:geonames c:islands\n","\n","10.381706422000207 0.5190853211000104\n","\n","10, b:Vendetta c:Films about fascism\n","\n","4.8466310759995395 0.48466310759995396\n","\n","20, t:Graveyard i:Goro Kishitani Shigenori Takechi\n","\n","9.610528437999164 0.4805264218999582\n","\n","5, t:Commonwealth Games i:melbourne b:Stolenwealth c:sports\n","\n","15.871963655999934 3.174392731199987\n","\n","20, b:Californication Season\n","\n","4.630024422998758 0.23150122114993793\n","\n","10, lesbouefs somme\n","\n","3.2032462209990626 0.32032462209990625\n","\n","10, b:Redmond i:Los Angeles Kings c:1965 births\n","\n","17.675101922000977 1.7675101922000978\n","\n","10, b:Dennis Lewiston c:1934 births\n","\n","9.297597004999261 0.9297597004999261\n","\n","10, engel's law\n","\n","7.614589659000558 0.7614589659000558\n","\n","5, b:Big Four i:Agatha Christie\n","\n","7.667683580000812 1.5335367160001625\n","\n","10, i:Andrey Savin b:Speedway World Cup\n","\n","9.696029253000233 0.9696029253000233\n","\n","10, Catcher in the Rye\n","\n","0.9202776970014384 0.09202776970014384\n","\n","20, b:Prisoner of Azkaban i:1999\n","\n","8.840110674998868 0.44200553374994345\n","\n","10, b:Prisoner of Azkaban i:2004\n","\n","8.079511419000482 0.8079511419000482\n","\n","3, b:V for Vendetta i:2006\n","\n","11.904341007999392 3.9681136693331305\n","\n","5, young's modulus\n","\n","4.948239151999587 0.9896478303999174\n","\n","10, b:Speed of light c:Concepts in physics\n","\n","10.320497709999472 1.0320497709999472\n","\n","20, b:angus r:tories i:James Lee\n","\n","7.473646371998257 0.37368231859991285\n","\n","30, b:Taxicab 1729\n","\n","5.721464095999181 0.19071546986663937\n","\n","50, b:Marc Spector i:Marvel Comics c:1980 comics debuts\n","\n","17.180883199000164 0.34361766398000326\n","\n","100, Sudebnik\n","\n","2.8293720960009523 0.028293720960009525\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lQ10pkX8g3XR"},"source":[""],"execution_count":null,"outputs":[]}]}