# -*- coding: utf-8 -*-
"""search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cN7co7pr0q7vAYCTSEqBQYR99gvvFH6u
"""

import xml.sax
import sys
sys.path.append('/content/drive/My Drive/IRE')
import collections
import os
import os.path
import re
from collections import defaultdict
import Stemmer
# from nltk.stem.snowball import SnowballStemmer
# from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
import timeit
from parsedata import Parser
from bisect import bisect
from math import log10
from operator import itemgetter

# !pip3 install PyStemmer
# !pip3 install nltk

index_file_path = ""
query_file_path = ""
output_file_path = ""
total_docs = 0
id_title = dict()
secondary_index = []
queries = []
outputs = []
query_result = dict()

def process_title_file(id_title_file_path):
  global total_docs, id_title
  with open(id_title_file_path, 'r') as inf:
    for l in inf:
      id, title = l.split(":",1)
      id_title[id] = title
      total_docs += 1

def read_secondary_index(secondary_index_file_path):
  global secondary_index
  with open(secondary_index_file_path, 'r') as inf:
    for l in inf:
      secondary_index.append(l.split(":")[0])

def fetch_data(check_match, data_in_file):
  start_in = check_match.start()
  find_in = start_in+1
  end_in = data_in_file.find('\n', find_in)
  get_data = data_in_file[start_in:end_in]
  return get_data.split(':')[1].split('|')

def read_primary_index_file(filename, word):
  primary_file = open(filename, 'r')
  data_in_file = primary_file.read()
  check_match = re.search('^'+word+':', data_in_file, re.M)
  if check_match:
    fetched_data = fetch_data(check_match, data_in_file)
    return fetched_data
  else:
    return []


def get_index_field_query(tag, word):
  global secondary_index, query_result, index_file_path, total_docs
  # print(word)
  # print(secondary_index)
  # print("word processed ",tag, word)
  indx = bisect(secondary_index, word)
  entire_list = list()
  if indx > 0:
    entire_list = read_primary_index_file(index_file_path+'primaryfile_'+str(indx)+'.txt', word)
  
  df = len(entire_list)
  idf = log10(total_docs/(1+df))

  for lt in entire_list:
    wt_freq = 0
    docid, posting_list = lt.split('#')
    docid = docid[1:]
    for cat in posting_list.split('+'):
      post_tag = cat[0]
      freq = int(cat[1:])
      if post_tag == tag:
        wt_freq += freq*10000
      else:
        wt_freq += freq

    tfidf = float(log10(1+wt_freq)*float(idf))
    if docid not in query_result:
      query_result[docid] = tfidf
    else:
      query_result[docid] += tfidf
    # print(docid, query_result[docid])
         


def process_field_query(tag, words):
  ex = re.compile(r'[\_]', re.DOTALL)
  words_ex = ex.sub(' ', words)
  words_list = re.findall("\d+|[\w]+", str(words_ex))
  words_list = clean_words(words_list)
  for w in words_list:
    get_index_field_query(tag, w.casefold())

def get_k_documents(k):
  global query_result, id_title, outputs
  cnt = 0
  dic_rev_sorted = dict(sorted(query_result.items(), key=itemgetter(1), reverse=True))
  for doc,title in dic_rev_sorted.items():
    outputs.append(doc+', '+id_title[doc])
    cnt += 1
    if cnt == k:
      break
  query_result.clear()

def process_multi_field_query(query):
  words = query.split(' ')
  local_dict = {}
  for i in words:
    if ':' in i:
      tag, w = i.split(':')
      local_dict[tag] = [w.casefold().strip()]
    else:
      local_dict[tag].append(i.casefold())
  return local_dict

def clean_words(words):
  #words = words.casefold()
  stop_words = set(stopwords.words('english'))
  tokens = [t for t in words if t not in stop_words]
  stemmer = Stemmer.Stemmer('english')
  tokens = [stemmer.stemWord(w) for w in tokens]
  return tokens

def get_index_normal_query(word):
  indx = bisect(secondary_index, word)
  entire_list = list()
  if indx > 0:
    entire_list = read_primary_index_file(index_file_path+'primaryfile_'+str(indx)+'.txt', word)
  
  df = len(entire_list)
  idf = log10(total_docs/(1+df))

  li = {'t':0.35, 'i':0.2, 'b':0.1, 'c':0.25, 'l':0.07, 'r':0.03}

  for lt in entire_list:
    wt_freq = 0
    docid, posting_list = lt.split('#')
    docid = docid[1:]
    for cat in posting_list.split('+'):
      post_tag = cat[0]
      freq = int(cat[1:])
      wt_freq += li[post_tag]*freq

    tfidf = float(log10(1+wt_freq)*float(idf))
    if docid not in query_result:
      query_result[docid] = tfidf
    else:
      query_result[docid] += tfidf


def process_normal_query(words):
  words = words.lower()
  ex = re.compile(r'[\_]', re.DOTALL)
  words_ex = ex.sub(' ', words)
  words_list = re.findall("\d+|[\w]+", str(words_ex))
  words_list = clean_words(words_list)
  # print(words_list)
  for word in words_list:
    get_index_normal_query(word)
  
  
def search_queries():
  global outputs, queries
  li = ['t:', 'i:', 'b:', 'c:', 'l:', 'r:']
  for q in queries:
      # outputs.append(q)
      num,qu = q.split(',',1)
      num = int(num)
      qu = qu.strip()
      start = timeit.default_timer()
      ## check for field query
      if any(val in qu for val in li):
        processed_dictionary = process_multi_field_query(qu)
        for tag,words in processed_dictionary.items():
          # print(tag, words)
          process_field_query(tag, ' '.join(words))
        get_k_documents(num)
        # print(outputs)
      else:
        process_normal_query(qu)
        get_k_documents(num)
        # print(outputs)
      stop = timeit.default_timer()
      outputs.append(str(stop-start)+' '+str((stop-start)/num)+'\n')
      outputs.append('\n')

if __name__=="__main__":
  global index_file_path, query_file_path, output_file_path, queries 
  index_file_path = "/content/drive/My Drive/IRE/InvertedIndexFiles/"
  query_file_path = '/content/sample_data/query.txt'
  output_file_path = '/content/drive/My Drive/IRE/serachtest/output.txt'
  id_title_file_path = '/content/drive/My Drive/IRE/IndexFiles/id_title.txt'
  process_title_file(id_title_file_path)

  secondary_index_file_path = '/content/drive/My Drive/IRE/InvertedIndexFiles/secondaryfile.txt'
  read_secondary_index(secondary_index_file_path)

  queries = open(query_file_path, 'r').readlines()
  search_queries()
  
  # print("time", stop-start)
  with open(output_file_path, 'w') as of:
    for line in outputs:
      of.write(line.strip()+'\n')

